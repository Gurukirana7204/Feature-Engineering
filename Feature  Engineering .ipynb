{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b72e3c-fd66-40f2-b13d-28940c722b58",
   "metadata": {},
   "source": [
    "Feature \n",
    "Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5fb1f-9437-4011-ae46-c9fdbd47c102",
   "metadata": {},
   "source": [
    "Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc21040-66a7-44db-a8a9-30b7b5787a12",
   "metadata": {},
   "source": [
    "1. What is a parameter?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e41411-0439-4f22-b6bd-02158ced13a1",
   "metadata": {},
   "source": [
    "In **Machine Learning (ML)**, a **parameter** refers to the internal variables or coefficients that are learned by the model during the training process. These parameters define the model and control how it makes predictions or classifications based on the input data.\n",
    "\n",
    "There are two key types of parameters in ML:\n",
    "\n",
    "1. **Model Parameters**:\n",
    "   - These are the values that are learned directly from the training data.\n",
    "   - They determine how the model maps inputs to outputs.\n",
    "   - In supervised learning, these parameters are adjusted during training to minimize the error or loss function.\n",
    "   - Example: In a **linear regression model** `y = wx + b`, the parameters are `w` (weight) and `b` (bias). These are the values that the algorithm learns during training to make accurate predictions.\n",
    "\n",
    "2. **Hyperparameters**:\n",
    "   - While not learned from the training data, hyperparameters are set before training and control the training process itself.\n",
    "   - Hyperparameters influence the model's learning ability and performance, but they are typically tuned through experimentation.\n",
    "   - Example: In a **neural network**, hyperparameters include the learning rate, number of layers, number of neurons in each layer, and the type of activation function.\n",
    "\n",
    "### Differences between Model Parameters and Hyperparameters:\n",
    "- **Model Parameters**: Learned by the algorithm during training (e.g., weights in a neural network).\n",
    "- **Hyperparameters**: Set manually before training begins and influence the learning process (e.g., learning rate, batch size).\n",
    "\n",
    "### Example:\n",
    "- In a **linear regression** model, the parameters `w` (weight) and `b` (bias) are learned from the data.\n",
    "- In a **decision tree** model, the parameters include the splits at each node based on the features, while hyperparameters would include the maximum depth of the tree or the minimum samples required to split a node.\n",
    "\n",
    "In summary, in ML, **parameters** refer to the internal model components that are adjusted through training to improve the model's performance on a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6353dc-d9d2-4038-a056-bad6ed4f233c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4628c82-bef9-41ee-b6e9-9651d91150e7",
   "metadata": {},
   "source": [
    "2. What is correlation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806dc38-6e2d-4a05-9cf6-0c7b9752adf2",
   "metadata": {},
   "source": [
    "\n",
    "In Machine Learning (ML), correlation refers to the statistical relationship between two or more variables, where one variable's value can be predicted based on another variable's value. Understanding correlation is essential in ML because it helps to identify patterns in data, feature relationships, and how different features influence the target variable.\n",
    "\n",
    "Role of Correlation in ML:\n",
    "Feature Selection:\n",
    "\n",
    "Correlation helps in selecting relevant features (input variables) for the model. If two features are highly correlated, one of them might be redundant and can be removed to improve model efficiency and avoid overfitting.\n",
    "For example, if you have two features, height and weight, which are highly correlated, you might only keep one of them in the model to avoid multicollinearity (when multiple features are highly correlated with each other).\n",
    "Understanding Relationships Between Features:\n",
    "\n",
    "Correlation allows you to understand how features are related to each other. In supervised learning, it is particularly useful to check how each feature correlates with the target variable.\n",
    "For instance, in predicting house prices, features like square footage and number of bedrooms might be positively correlated with the target variable, price.\n",
    "Reducing Multicollinearity:\n",
    "\n",
    "Multicollinearity occurs when two or more features are highly correlated, making it difficult to separate their individual contributions to the outcome. High multicollinearity can destabilize regression models, making it harder to interpret the effect of individual features.\n",
    "By identifying correlated features, you can eliminate or combine them to reduce multicollinearity.\n",
    "Improving Model Performance:\n",
    "\n",
    "Knowing the correlation between features helps in creating better predictive models. If a feature is highly correlated with the target variable, it will likely contribute significantly to the model’s performance.\n",
    "In regression, for example, the model can use highly correlated features to make more accurate predictions.\n",
    "Types of Correlation in ML:\n",
    "Pearson Correlation: Measures the linear relationship between two continuous variables. It gives a value between -1 and +1, where:\n",
    "+1: Perfect positive correlation\n",
    "-1: Perfect negative correlation\n",
    "0: No linear correlation\n",
    "Spearman's Rank Correlation: Measures the relationship between two variables based on their ranks (non-parametric). It’s useful when the relationship is not necessarily linear but still monotonic.\n",
    "Kendall's Tau: Another rank-based correlation measure, often used for ordinal data or when the relationship is less clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36874fef-6a08-44cb-bc1c-bb33b9685cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cbb1a85-a164-439c-b842-47789ce8bb09",
   "metadata": {},
   "source": [
    "2. What does negative correlation mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96424b08-e50e-45e7-b0f5-103a5d657434",
   "metadata": {},
   "source": [
    "Negative correlation refers to a relationship between two variables where, as one variable increases, the other variable tends to decrease, and vice versa. In other words, the two variables move in opposite directions.\n",
    "\n",
    "Key Characteristics of Negative Correlation:\n",
    "Inverse Relationship: When one variable goes up, the other goes down, and vice versa.\n",
    "Correlation Coefficient: The correlation coefficient for negative correlation is between 0 and -1. A coefficient of:\n",
    "-1 indicates a perfect negative correlation, meaning that as one variable increases, the other decreases in a perfectly predictable manner.\n",
    "A coefficient closer to 0 (but still negative) indicates a weaker negative correlation, meaning the relationship is still inverse but less predictable.\n",
    "Example of Negative Correlation:\n",
    "Height and Distance from the Ground: If you are measuring the height of a person and their distance from the ground (such as the length of a person’s shadow), there would be a negative correlation. As the height of a person increases, the distance from the ground (if measured as the inverse of height) decreases.\n",
    "Temperature and Hot Chocolate Sales: In colder weather, people might buy more hot chocolate. Thus, as temperature decreases, hot chocolate sales increase, showing a negative correlation between temperature and sales.\n",
    "Real-World Example:\n",
    "Exercise and Weight: There is often a negative correlation between the amount of physical exercise and body weight. As the amount of exercise increases, body weight tends to decrease, assuming diet and other factors remain constant.\n",
    "Correlation Coefficient Interpretation:\n",
    "+1: Perfect positive correlation (both variables increase together).\n",
    "0: No correlation (no consistent relationship).\n",
    "-1: Perfect negative correlation (one variable increases while the other decreases in a perfectly predictable way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50069ac0-7e69-490f-bafe-00107b8bbc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6018f1e5-5f37-4731-8249-49e2828f6c26",
   "metadata": {},
   "source": [
    "3. Define Machine Learning. What are the main components in Machine Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a64cc9-ce3e-40f0-b332-8240a86307f1",
   "metadata": {},
   "source": [
    "Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to learn from data and make predictions or decisions without being explicitly programmed for each task. Instead of following strict, pre-defined rules, machine learning systems improve their performance by identifying patterns in data and adjusting based on new information.\n",
    "\n",
    "Summary of the Main Components:\n",
    "\n",
    "Data (input and target values), \n",
    "Algorithms (procedures to learn from data), \n",
    "Model (output of the learning process), \n",
    "Features (attributes of the data), \n",
    "Training (learning phase where the model adjusts its parameters), \n",
    "Evaluation (measuring model performance), \n",
    "Optimization (tuning hyperparameters and improving the model), \n",
    "Prediction (applying the model to new data), \n",
    "Feedback and Iteration (continuous improvement through retraining and fine-tuning), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb53ec-e9a0-4955-a2bd-9b8e20101e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07617168-81aa-4fd7-af5d-6e42f92b74d4",
   "metadata": {},
   "source": [
    "4. How does loss value help in determining whether the model is good or not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf32fe-1c49-4bfe-b26e-96b84ef0cf2a",
   "metadata": {},
   "source": [
    "\n",
    "The loss value (also referred to as the loss function or cost function) is a crucial metric in machine learning that helps determine how well a model's predictions match the actual target values. It is used during training to guide the optimization process, and it provides insight into the model's performance.\n",
    "\n",
    "Role of the Loss Value in Determining Model Quality:\n",
    "Quantifying Prediction Error:\n",
    "\n",
    "The loss function calculates the error between the predicted output (from the model) and the actual output (from the ground truth or target values).\n",
    "The lower the loss value, the better the model's predictions are, because it indicates that the model's predictions are closer to the true values.\n",
    "Guiding Model Optimization:\n",
    "\n",
    "During training, the goal is to minimize the loss. Optimizing the model involves adjusting its parameters (such as weights in a neural network or coefficients in a regression model) to minimize the loss function, typically using an optimization algorithm like gradient descent.\n",
    "By continuously reducing the loss, the model becomes better at making accurate predictions.\n",
    "Indicating Overfitting or Underfitting:\n",
    "\n",
    "Underfitting: If the model is not complex enough or hasn't learned the patterns in the data, the loss value may remain high during training and testing. This indicates that the model has not captured the underlying structure of the data.\n",
    "Overfitting: If the model is too complex and learns the noise or specific details of the training data, the loss value on the training set might be very low, but the loss on the test set (unseen data) will be high. This shows that the model has memorized the training data but doesn't generalize well to new data.\n",
    "Evaluating Different Models:\n",
    "\n",
    "Loss values allow for comparison between different models or different versions of a model. A model with a lower loss value is typically considered to be more effective at making predictions.\n",
    "For instance, when tuning hyperparameters or testing different algorithms (e.g., logistic regression vs. decision trees), the model that results in the lowest loss is generally the preferred one.\n",
    "Different Loss Functions for Different Tasks:\n",
    "\n",
    "Different types of machine learning tasks require different loss functions. For example:\n",
    "Regression: Mean Squared Error (MSE) or Mean Absolute Error (MAE) are commonly used to measure the difference between predicted and actual continuous values.\n",
    "Classification: Cross-entropy loss is often used to evaluate how well the model classifies categorical data (such as binary or multi-class classification problems).\n",
    "Reinforcement Learning: The loss is related to the reward feedback the model receives after taking actions in an environment.\n",
    "The choice of loss function directly influences the model's performance and how it is trained.\n",
    "Performance Metrics:\n",
    "\n",
    "While the loss value itself tells you how well a model is fitting the data, it doesn't always tell you everything about model quality. In classification problems, for example, you may also need to consider accuracy, precision, recall, or F1-score.\n",
    "However, the loss is a foundational metric used to guide training, and improving it often leads to better overall performance on other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b3aa0-bd6f-40f5-9986-b0750cb2a620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02a86afd-f0cf-4f59-8322-c5a58e557541",
   "metadata": {},
   "source": [
    "5. What are continuous and categorical variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5518f7-4a14-466a-9239-898bfa44c43d",
   "metadata": {},
   "source": [
    "Continuous and categorical variables are two key types of data used in statistical analysis and machine learning. They represent different types of information and require different methods of analysis.\n",
    "\n",
    "1. Continuous Variables:\n",
    "A continuous variable is a type of quantitative variable that can take an infinite number of values within a given range. These variables are typically measured and can represent any real number, including decimals or fractions. They can assume an infinite number of possible values between two points.\n",
    "\n",
    "Characteristics of Continuous Variables:\n",
    "Infinite Possible Values: Continuous variables can take any value within a range, and their values can be as precise as the measurement allows.\n",
    "Measurable: These variables are typically measured with instruments, such as a thermometer or a scale, and can represent quantities like height, weight, time, or temperature.\n",
    "Mathematical Operations: You can perform mathematical operations like addition, subtraction, multiplication, and division on continuous variables.\n",
    "Examples of Continuous Variables:\n",
    "Height: A person's height could be 170.5 cm, 170.55 cm, or 170.555 cm, and the precision depends on how accurately it is measured.\n",
    "Temperature: Temperature can be 30°C, 30.1°C, 30.01°C, etc.\n",
    "Salary: A person's salary could be $50,000, $50,500, or $50,000.25.\n",
    "Time: Time can be measured to hours, minutes, seconds, or even milliseconds.\n",
    "\n",
    "2. Categorical Variables:\n",
    "A categorical variable is a type of qualitative variable that represents categories or groups. These variables can take on a limited, fixed number of values, each of which represents a distinct category or label.\n",
    "\n",
    "Characteristics of Categorical Variables:\n",
    "Limited Number of Categories: Categorical variables take a small number of distinct values or categories, with no inherent ordering.\n",
    "Qualitative: These variables often describe qualities or characteristics, such as a color, type, or class.\n",
    "Non-Numeric: In many cases, categorical variables are non-numeric, though they can be encoded as numbers for convenience in some analyses (e.g., 1 for \"male\" and 0 for \"female\").\n",
    "Types of Categorical Variables:\n",
    "Nominal Variables: These are categories without any meaningful order. The values represent different groups, but the order or ranking doesn't matter.\n",
    "\n",
    "Examples:\n",
    "Gender: Male, Female, Non-binary\n",
    "Color: Red, Blue, Green\n",
    "Car Brands: Toyota, Honda, Ford\n",
    "Ordinal Variables: These are categories with a meaningful order or ranking, but the intervals between the categories are not necessarily equal.\n",
    "\n",
    "Examples:\n",
    "Education Level: High School, Bachelor's, Master's, PhD\n",
    "Rating Scale: Poor, Average, Good, Excellent\n",
    "Customer Satisfaction: Very Unsatisfied, Unsatisfied, Neutral, Satisfied, Very Satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901e498-74fb-4d4d-9b36-e3570fe03a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27f6cbac-27bf-42f6-b963-a277c93a1747",
   "metadata": {},
   "source": [
    "6. How do we handle categorical variables in Machine Learning? What are the common t\n",
    "echniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596da95e-8ef6-45e0-bdaa-22ac222c0b4f",
   "metadata": {},
   "source": [
    "Handling categorical variables in machine learning is crucial, as many algorithms expect numerical input data. Categorical variables represent labels or categories, and machine learning models typically can't work directly with non-numeric data. Therefore, we need to convert categorical variables into a numerical format.\n",
    "\n",
    "There are several techniques to handle categorical data, and the choice of technique depends on the nature of the data (whether it's nominal or ordinal) and the machine learning algorithm being used.\n",
    "\n",
    "Common Techniques to Handle Categorical Variables:\n",
    "1. Label Encoding:\n",
    "Label Encoding is the process of converting each category into a unique integer. This method is often used when the categorical variable has an ordinal relationship (i.e., there is a meaningful order or ranking among the categories).\n",
    "\n",
    "Example:\n",
    "Colors: Red, Green, Blue → Red = 0, Green = 1, Blue = 2\n",
    "Education Level: High School, Bachelor’s, Master’s → High School = 0, Bachelor’s = 1, Master’s = 2\n",
    "Use Case:\n",
    "Best for ordinal categorical variables (variables with a natural order).\n",
    "It may not be ideal for nominal variables because the model may interpret the numerical values as having an inherent order or priority (e.g., Blue being ranked as 2 might imply it is \"more\" than Red).\n",
    "\n",
    "2. One-Hot Encoding:\n",
    "One-Hot Encoding is a technique that converts categorical variables into binary (0 or 1) columns, each representing a category. This method is ideal for nominal categorical variables where there is no meaningful order.\n",
    "\n",
    "Example:\n",
    "For a variable \"Color\" with categories Red, Green, and Blue, one-hot encoding would create three new binary features:\n",
    "\n",
    "Red: [1, 0, 0]\n",
    "Green: [0, 1, 0]\n",
    "Blue: [0, 0, 1]\n",
    "Use Case:\n",
    "Best for nominal categorical variables (no inherent order).\n",
    "It creates multiple columns, which can increase dimensionality, especially when there are many categories, leading to higher memory consumption and potential overfitting. This is sometimes referred to as the \"curse of dimensionality.\"\n",
    "\n",
    "3. Ordinal Encoding:\n",
    "Ordinal encoding is similar to label encoding, but it specifically caters to ordinal variables (variables with a natural, ordered relationship). Unlike label encoding, where the values are arbitrary integers, ordinal encoding ensures the ordering of categories reflects their inherent rank.\n",
    "\n",
    "Example:\n",
    "For a variable \"Education Level\" with categories:\n",
    "\n",
    "High School, Bachelor’s, Master’s\n",
    "The ordinal encoding might assign the following values:\n",
    "High School: 0\n",
    "Bachelor’s: 1\n",
    "Master’s: 2\n",
    "Use Case:\n",
    "Ideal for ordinal categorical variables, where the categories have a meaningful order but no precise numerical distance between them.\n",
    "It maintains the rank order, but like label encoding, it might still introduce some bias if the model interprets the encoding as having a numeric distance.\n",
    "\n",
    "4. Target Encoding (Mean Encoding):\n",
    "Target encoding involves replacing each category with the mean of the target variable for that category. It is particularly useful when there are many categories, as one-hot encoding can lead to sparse data (high-dimensional data with many 0s).\n",
    "\n",
    "Example:\n",
    "If you're predicting house prices and you have a categorical variable like \"Neighborhood\", target encoding would replace each neighborhood with the average house price for that neighborhood.\n",
    "Neighborhood A might have an average price of $300,000.\n",
    "Neighborhood B might have an average price of $400,000.\n",
    "Use Case:\n",
    "Best for high-cardinality categorical variables (variables with many categories).\n",
    "Can lead to overfitting if the number of observations per category is small, as it might fit the noise. This can be mitigated by adding regularization.\n",
    "\n",
    "5. Binary Encoding:\n",
    "Binary encoding is a compromise between one-hot encoding and label encoding. It converts the categorical values into binary numbers, then splits them into separate columns.\n",
    "\n",
    "Example:\n",
    "For a categorical variable with 6 unique categories:\n",
    "\n",
    "First, each category is assigned a unique integer value (like label encoding).\n",
    "Then, these integers are converted to binary.\n",
    "Category 0 → 000\n",
    "Category 1 → 001\n",
    "Category 2 → 010\n",
    "Category 3 → 011\n",
    "Category 4 → 100\n",
    "Category 5 → 101\n",
    "This method reduces dimensionality compared to one-hot encoding while still keeping the data in a usable format.\n",
    "\n",
    "Use Case:\n",
    "Best for high-cardinality categorical variables where one-hot encoding would lead to too many new features.\n",
    "It helps reduce the dimensionality compared to one-hot encoding, while still encoding the category information in a binary format.\n",
    "\n",
    "6. Frequency or Count Encoding:\n",
    "In frequency encoding, each category is replaced by the frequency (or count) of how often it appears in the dataset. This method is particularly useful for variables with a large number of categories.\n",
    "\n",
    "Example:\n",
    "If you have a categorical variable \"City\" and it contains the following:\n",
    "\n",
    "City A: appears 30 times\n",
    "City B: appears 20 times\n",
    "City C: appears 50 times\n",
    "Each city is replaced with its count:\n",
    "\n",
    "City A → 30\n",
    "City B → 20\n",
    "City C → 50\n",
    "Use Case:\n",
    "Best for high-cardinality variables where categories may be related to the target in a frequency-based manner.\n",
    "It can be problematic if the frequencies are not well correlated with the target variable, leading to less interpretability.\n",
    "\n",
    "7. Embedding Layers (for Neural Networks):\n",
    "For complex categorical variables with high cardinality, especially when dealing with deep learning models (e.g., neural networks), embedding layers can be used. Embeddings map categorical variables to a dense vector representation, which allows the model to learn relationships between categories during training.\n",
    "\n",
    "Use Case:\n",
    "Best for high-cardinality variables in deep learning models, such as word embeddings for natural language processing (NLP) tasks.\n",
    "This technique helps reduce dimensionality and captures complex relationships between categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81edc769-ee8d-4fcb-af83-2a7698f80ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72701554-d7dc-4919-8f77-d36c59a80376",
   "metadata": {},
   "source": [
    "7. What do you mean by training and testing a dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90739f3-6961-4c44-9752-8ddeb4448201",
   "metadata": {},
   "source": [
    "In machine learning, training and testing a dataset refer to the process of developing a model by teaching it from a set of data (training) and then evaluating its performance on unseen data (testing). These two steps are crucial for building a model that generalizes well to new, unseen data and not just the data it was trained on.\n",
    "\n",
    "1. Training a Dataset:\n",
    "Training refers to the process of feeding data into the machine learning model and allowing it to learn from this data. The model uses the training dataset to identify patterns, relationships, or structure in the data that it can later use to make predictions on new data.\n",
    "\n",
    "Key Steps in Training:\n",
    "\n",
    "Model Selection: Choose the type of model (e.g., decision tree, neural network, regression) based on the problem.\n",
    "\n",
    "Data Preparation: Clean, preprocess, and transform the data into a format suitable for the model.\n",
    "\n",
    "Learning Process: The model learns from the training data by adjusting its internal parameters (e.g., weights in a neural network or coefficients in a \n",
    "linear regression) to minimize the error between its predictions and the true values.\n",
    "\n",
    "Optimization: An optimization algorithm (such as gradient descent) is used to minimize the loss function, which quantifies how far the model's predictions are from the actual outcomes. This is done iteratively across the dataset.\n",
    "During training, the model is \"fit\" to the data, meaning that it tries to learn a mapping from the input features (independent variables) to the output label or target (dependent variable).\n",
    "\n",
    "Example:\n",
    "In a supervised learning problem like predicting house prices (where the target variable is the house price and the features could be square footage, number of bedrooms, etc.), training the model would involve providing it with many examples of houses and their corresponding prices. The model then learns the relationship between the features and the target.\n",
    "\n",
    "2. Testing a Dataset:\n",
    "Testing refers to evaluating the model's performance on data that it hasn't seen before during training. The testing dataset is used to assess how well the model generalizes to new, unseen data and whether it performs accurately outside of the training environment.\n",
    "\n",
    "Key Steps in Testing:\n",
    "\n",
    "Performance Evaluation: After training the model, it is tested using a separate set of data (the test dataset) to determine how well it can predict or classify unseen examples.\n",
    "\n",
    "Metrics Calculation: The model's predictions are compared to the actual labels in the test dataset using performance metrics such as:\n",
    "Accuracy (for classification tasks)\n",
    "Mean Squared Error (MSE) (for regression tasks)\n",
    "Precision, Recall, F1-score (for classification tasks)\n",
    "AUC-ROC Curve (for classification tasks)\n",
    "Testing helps to identify overfitting (where the model performs well on the training data but poorly on the test data) and underfitting (where the model performs poorly on both the training and testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad911f-f7fd-4e72-9ed1-c12227dad44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "438409dc-f3f8-4685-8c77-275f0c07ff44",
   "metadata": {},
   "source": [
    "8. What is sklearn.preprocessing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e2c86-2352-49d1-a700-1b995ed16c82",
   "metadata": {},
   "source": [
    "**sklearn.preprocessing** is a module in the scikit-learn library (often abbreviated as sklearn) that provides a set of functions and classes for preprocessing data before applying machine learning algorithms. Preprocessing is a crucial step in the machine learning pipeline as it ensures the data is in the right format, scales, and is ready for use with machine learning models.\n",
    "\n",
    "This module includes tools to perform operations like scaling, normalization, encoding categorical variables, imputing missing values, and more. Proper preprocessing helps improve model performance and ensures that the machine learning model can make accurate predictions.\n",
    "\n",
    "Here are some key features and functions from sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc9974-1b0b-495a-b0df-88560dcdfdd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed253a59-c639-4b6a-b268-6b9a95c48ed5",
   "metadata": {},
   "source": [
    "9. What is a Test set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f611a-cbea-483f-b51b-3de113e5cacf",
   "metadata": {},
   "source": [
    "A test set in machine learning is a portion of the dataset that is used to evaluate the performance of a trained model. It contains data that the model has never seen during training, meaning it is independent of the data used to train the model. The purpose of the test set is to assess how well the model generalizes to new, unseen data and to estimate its performance on real-world, out-of-sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2864cb11-2865-4619-abb7-11cf17400e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b47d76e-1aaf-4a56-8926-a22b7aaf428a",
   "metadata": {},
   "source": [
    "10. How do we split data for model fitting (training and testing) in Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7f4a7-9b10-4fcd-8dc0-eed8033aff1d",
   "metadata": {},
   "source": [
    "In Python, scikit-learn (or sklearn) provides an easy and efficient way to split data into training and testing sets using the train_test_split() function. This function allows you to divide your dataset into two or more subsets (such as training and testing) in a random and reproducible way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5c0d05f-38d4-425c-b48f-8b5f67526767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (120, 4)\n",
      "Testing set size: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the resulting datasets\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a911e-dafe-4270-a964-250d913a5ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ac5864b-8010-4e8b-aa65-6dcead219fd8",
   "metadata": {},
   "source": [
    "10. How do you approach a Machine Learning problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6da70-2549-4129-9d9f-91889bc80e0e",
   "metadata": {},
   "source": [
    "Approaching a Machine Learning (ML) problem involves a series of steps that guide you from understanding the problem to deploying a model that can make accurate predictions on unseen data. Here's a structured approach to tackle an ML problem:\n",
    "1. Understand the Problem\n",
    "2. Data Collection\n",
    "3. Data Preprocessing and Exploration\n",
    "4. Split the Data\n",
    "5. Model Selection\n",
    "6. Train the Model\n",
    "7. Evaluate the Model\n",
    "8. Model Improvement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27e759-bcd0-401b-a18a-fae58a4bb1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43a837e0-dd6d-4cc4-9d7a-a8ade99aa939",
   "metadata": {},
   "source": [
    "11. Why do we have to perform EDA before fitting a model to the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b136a59-a92f-45ad-acd4-d6507fe6e96d",
   "metadata": {},
   "source": [
    "Performing Exploratory Data Analysis (EDA) before fitting a model to the data is crucial for several reasons. EDA helps you understand the dataset, identify potential issues, and make informed decisions during the modeling process. Here's why EDA is an essential step:\n",
    "1. Understanding the Data\n",
    "2. Data Quality Check\n",
    "3. Distribution of Data\n",
    "4. Identifying Relationships and Patterns\n",
    "5. Feature Engineering Insights\n",
    "6. Detecting Class Imbalances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f70df-c400-49da-ac60-ddf7b9912145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ef158aa-c2bd-4415-ad6e-e4e31a7f1530",
   "metadata": {},
   "source": [
    "14. How can you find correlation between variables in Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017671fa-2a74-4d93-b208-d321c9efe49f",
   "metadata": {},
   "source": [
    "In Python, you can find the correlation between variables using several methods. The most common method is to use Pandas and NumPy to compute correlation matrices and visualize the relationships between variables. Below are the steps you can follow to calculate correlation between variables:\n",
    "\n",
    "1. Using Pandas: DataFrame.corr()\n",
    "The corr() method in Pandas is used to compute the correlation matrix of a DataFrame, which shows the pairwise correlation between numerical variables.\n",
    "\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [5, 4, 3, 2, 1],\n",
    "    'C': [1, 3, 5, 7, 9]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "print(correlation_matrix)\n",
    "Output:\n",
    "css\n",
    "Copy code\n",
    "          A    B    C\n",
    "A  1.000000 -1.000000  1.000000\n",
    "B -1.000000  1.000000 -1.000000\n",
    "C  1.000000 -1.000000  1.000000\n",
    "Correlation Value Interpretation:\n",
    "1: Perfect positive correlation.\n",
    "-1: Perfect negative correlation.\n",
    "0: No correlation.\n",
    "A value between -1 and 1 indicates a degree of linear correlation.\n",
    "2. Using Seaborn for Visualization: heatmap()\n",
    "To visualize the correlation matrix, you can use Seaborn's heatmap function. This provides a graphical representation of the correlations between variables.\n",
    "\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "This will display a heatmap where the color intensity represents the correlation, and the numerical values indicate the strength of the correlation between pairs of variables.\n",
    "\n",
    "3. Using NumPy: numpy.corrcoef()\n",
    "You can also use NumPy's corrcoef() function to calculate the correlation coefficient between two or more arrays.\n",
    "\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "\n",
    "# Define two arrays (variables)\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([5, 4, 3, 2, 1])\n",
    "\n",
    "# Calculate the correlation coefficient between x and y\n",
    "correlation = np.corrcoef(x, y)\n",
    "\n",
    "print(correlation)\n",
    "Output:\n",
    "lua\n",
    "Copy code\n",
    "[[ 1. -1.]\n",
    " [-1.  1.]]\n",
    "The matrix shows the correlation coefficient of 1 for x with itself and -1 for x with y (a perfect negative correlation).\n",
    "4. Pearson, Spearman, and Kendall Correlations\n",
    "Pandas' corr() method computes the Pearson correlation by default, but you can also compute other types of correlations like Spearman (non-parametric) or Kendall.\n",
    "\n",
    "Example of calculating different correlation types:\n",
    "python\n",
    "Copy code\n",
    "# Pearson correlation (default)\n",
    "pearson_corr = df.corr(method='pearson')\n",
    "\n",
    "# Spearman correlation\n",
    "spearman_corr = df.corr(method='spearman')\n",
    "\n",
    "# Kendall correlation\n",
    "kendall_corr = df.corr(method='kendall')\n",
    "\n",
    "print(\"Pearson Correlation:\\n\", pearson_corr)\n",
    "print(\"\\nSpearman Correlation:\\n\", spearman_corr)\n",
    "print(\"\\nKendall Correlation:\\n\", kendall_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c1ef3-9558-414d-8a8c-f291e066f9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f56d99f-df52-45b0-9807-54c6f908d2b5",
   "metadata": {},
   "source": [
    "15. What is causation? Explain difference between correlation and causation with an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e17d2-2464-4700-b269-6f600b0b6848",
   "metadata": {},
   "source": [
    "Causation refers to a relationship where one event or variable directly causes another. In other words, causation implies that changes in one variable directly bring about changes in another. When there is causation, we can say that X causes Y, meaning that X is responsible for changes in Y.\n",
    "\n",
    "Causal Relationship: If Variable X causes Variable Y, then a change in X will result in a change in Y, and this cause-and-effect relationship is often based on some mechanism or theory.\n",
    "For example, smoking causes lung cancer: Smoking (X) is the direct cause of lung cancer (Y). When a person smokes, they are at a higher risk of developing lung cancer.\n",
    "\n",
    "Difference Between Correlation and Causation\n",
    "\n",
    "While correlation and causation are related concepts, they are fundamentally different. Here’s an explanation of both, with an example to highlight their difference.\n",
    "\n",
    "1. Correlation\n",
    "\n",
    "Correlation measures the statistical relationship between two variables. It shows whether the variables move together in some way (either in the same direction or opposite directions). However, correlation does not imply that one variable causes the other to change.\n",
    "\n",
    "Correlation simply tells us that two variables are related in some way, but it doesn't tell us why or how they are related.\n",
    "Correlation can be positive, negative, or zero:\n",
    "Positive correlation: As one variable increases, the other increases.\n",
    "Negative correlation: As one variable increases, the other decreases.\n",
    "Zero correlation: No linear relationship between the variables.\n",
    "Example of Correlation:\n",
    "Ice Cream Sales and Drowning Incidents: A study might show that ice cream sales and drowning incidents have a positive correlation. As ice cream sales go up, drowning incidents also increase.\n",
    "While this may seem like ice cream consumption is contributing to drowning (which is absurd), the correlation does not imply causation. There’s a hidden variable at play — summer temperature. In the summer, more people buy ice cream and more people swim, which increases the likelihood of drowning incidents. Thus, the relationship is coincidental, not causal.\n",
    "\n",
    "2. Causation\n",
    "\n",
    "Causation means that one event or variable directly causes another. This implies that one variable has a direct impact on another. Causation is typically supported by scientific experiments, theory, or long-term observational studies that show a cause-and-effect relationship.\n",
    "\n",
    "Causation tells us that X causes Y, meaning the change in X leads to a change in Y.\n",
    "Example of Causation:\n",
    "Smoking and Lung Cancer: There is a well-established causal relationship between smoking and lung cancer. Studies show that smoking directly causes mutations in lung cells, which leads to the development of cancer.\n",
    "In this case, smoking (X) causes lung cancer (Y). The relationship is not just a coincidence but is supported by scientific evidence and biological mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0959a-38ea-4159-bf7a-38754feb5180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b80333b2-a906-4312-a967-2df123a40a4f",
   "metadata": {},
   "source": [
    "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139dc6e-0563-44f6-bb90-e97162349ec7",
   "metadata": {},
   "source": [
    "An optimizer in machine learning is an algorithm or method used to minimize (or maximize) a function by adjusting the parameters (weights and biases) of a model in order to improve its performance. Specifically, optimizers are used to minimize the loss function during the training of a model. The loss function measures how far off the model's predictions are from the actual results, and by minimizing this loss, the optimizer helps the model learn the best parameters.\n",
    "\n",
    "In simpler terms, the optimizer helps the model to \"learn\" by iteratively adjusting its parameters (weights) to reduce the error between the predicted and actual outputs.\n",
    "\n",
    "Types of Optimizers\n",
    "\n",
    "1. Gradient Descent\n",
    "\n",
    "Gradient Descent is the most basic and widely used optimization algorithm. It aims to find the minimum of the loss function by updating the model’s parameters in the opposite direction of the gradient (the derivative) of the loss function with respect to the parameters.\n",
    "\n",
    "Types of Gradient Descent:\n",
    "\n",
    "Batch Gradient Descent\n",
    "Stochastic Gradient Descent (SGD)\n",
    "Mini-batch Gradient Descent\n",
    "Working:\n",
    "\n",
    "The gradient of the loss function is calculated, and the model parameters are updated in the direction opposite to the gradient. The size of the step is determined by the learning rate.\n",
    "Example:\n",
    "\n",
    "Imagine you are trying to find the lowest point of a hill (the minimum of the loss function). Gradient descent will help you \"walk down\" the hill by taking steps based on the steepness (gradient) at each point, iterating until you reach the bottom.\n",
    "\n",
    "2. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent is a variant of gradient descent in which the model parameters are updated after evaluating the gradient for each individual training example. This leads to faster updates since it doesn’t wait for the full dataset to be processed.\n",
    "\n",
    "Working:\n",
    "\n",
    "Instead of calculating the gradient over the entire dataset (as in batch gradient descent), SGD updates the parameters for each sample, making the process faster but noisier. This noise can help the model escape local minima and reach a better solution.\n",
    "\n",
    "Example:\n",
    "\n",
    "In a large dataset, using batch gradient descent would require computing the gradient for all data points, which might be slow. SGD speeds this up by updating the weights after looking at each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91678b4-c80c-4a43-ab50-c1f2d1a76f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf4bcaa2-bfc7-4f03-95eb-5aa8a160503e",
   "metadata": {},
   "source": [
    "17. What is sklearn.linear_model ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10bcb6a-3754-4d27-8da7-cffde9202dcf",
   "metadata": {},
   "source": [
    "sklearn.linear_model is a module within Scikit-learn, a popular machine learning library in Python. This module provides a variety of algorithms for linear modeling, which are used to model the relationship between input features (independent variables) and a target variable (dependent variable) using linear equations. These algorithms are widely used for both regression (predicting continuous outcomes) and classification (predicting categorical outcomes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e603ee-b3d7-4f80-aec4-e0d4e3ea9943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "524737ac-2ce7-4f5a-ba68-59bed5c57315",
   "metadata": {},
   "source": [
    "18. What does model.fit() do? What arguments must be given?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4523932-7453-4b5e-a8ca-1a0bebc44fa5",
   "metadata": {},
   "source": [
    "What does model.fit() do?\n",
    "In machine learning, the fit() method is used to train a model on a given dataset. It adjusts the model’s internal parameters (like weights or coefficients) to learn patterns from the data. Essentially, calling fit() means you are providing the model with both input data (features) and the corresponding target values (labels) so that the model can learn the relationship between them.\n",
    "\n",
    "What happens when fit() is called?\n",
    "Input Data Processing:\n",
    "\n",
    "The model receives the training data, which consists of:\n",
    "\n",
    "X (features): The independent variables, or input features.\n",
    "y (target): The dependent variable, or labels, that we want to predict.\n",
    "\n",
    "Model Training:\n",
    "\n",
    "The model uses the input features (X) and the corresponding target values (y) to learn the relationship between them.\n",
    "Depending on the model type (e.g., linear regression, decision tree), it adjusts the model's parameters (like coefficients or weights) to minimize the error or loss function during training.\n",
    "\n",
    "Parameters Update:\n",
    "\n",
    "The internal parameters (weights, coefficients, etc.) of the model are updated during the training process so that the model can predict better on unseen data.\n",
    "\n",
    "What arguments must be given to fit()?\n",
    "X (features): A 2D array-like or pandas DataFrame that contains the input features. Each row represents a data point, and each column represents a feature or attribute.\n",
    "\n",
    "Shape: (n_samples, n_features)\n",
    "Example: For a dataset with 3 features and 100 data points, the shape would be (100, 3).\n",
    "y (target): A 1D array-like or pandas Series containing the target labels or values corresponding to each data point in X.\n",
    "\n",
    "Shape: (n_samples,)\n",
    "Example: For the same dataset with 100 data points, the shape of y would be (100,).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8127964a-25eb-4e6a-89f1-f36dddd0509e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e44b06fc-b41a-47e7-a221-568a7bc48177",
   "metadata": {},
   "source": [
    "19. What does model.predict() do? What arguments must be given?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025726cf-2954-4a7c-a01c-b3d89fb5c05b",
   "metadata": {},
   "source": [
    "The model.predict() method in machine learning is used to make predictions based on the model after it has been trained using the fit() method. Essentially, predict() applies the learned parameters (weights, coefficients, etc.) of the model to new, unseen data in order to predict outcomes.\n",
    "\n",
    "In regression tasks, predict() will return continuous values (e.g., predicted house prices).\n",
    "In classification tasks, predict() will return predicted class labels (e.g., whether an email is spam or not).\n",
    "What happens when predict() is called?\n",
    "\n",
    "Input Data:\n",
    "\n",
    "The method takes input data (features) in the same format as the data used for training.\n",
    "The model uses the learned relationships from the training phase to generate predictions for the new data.\n",
    "\n",
    "Prediction Process:\n",
    "\n",
    "For regression models, it computes a predicted continuous output.\n",
    "For classification models, it computes a predicted class label based on the input features.\n",
    "\n",
    "Output:\n",
    "\n",
    "The output of predict() will be the model’s predicted values (either continuous or discrete).\n",
    "What arguments must be given to predict()?\n",
    "\n",
    "X (features): A 2D array-like or pandas DataFrame containing the input features for which we want to make predictions.\n",
    "Shape: (n_samples, n_features)\n",
    "n_samples is the number of samples (data points) for which predictions are needed.\n",
    "n_features should match the number of features used during training (same as the X passed to fit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923dca6-6f5d-498f-8b12-07f497e7b2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6422c3a0-1027-4d5e-aa07-196e45496e4d",
   "metadata": {},
   "source": [
    "21. What is feature scaling? How does it help in Machine Learning?\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f995e-db67-406c-8f50-10808b8afcb3",
   "metadata": {},
   "source": [
    "Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in a dataset. The goal is to ensure that all features contribute equally to the model, preventing features with larger numerical ranges from dominating those with smaller numerical ranges during the learning process.\n",
    "\n",
    "Why is Feature Scaling Important?\n",
    "\n",
    "In machine learning, many algorithms (especially distance-based models like k-Nearest Neighbors and gradient-based models like linear regression) are sensitive to the scale of the data. If the features have different scales, the model may perform poorly because it may give more importance to the features with larger values.\n",
    "\n",
    "Benefits of Feature Scaling:\n",
    "\n",
    "Improved Model Performance:\n",
    "Models like k-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Gradient Descent-based algorithms (e.g., logistic regression, neural networks) rely on the distances between data points. If the features are not scaled, the distance calculations will be biased towards features with larger scales, reducing the model's accuracy.\n",
    "\n",
    "Faster Convergence in Gradient Descent:\n",
    "For algorithms that use gradient descent to minimize the loss function (like linear regression or logistic regression), feature scaling can help the algorithm converge faster. This is because the gradient steps will be more uniform, improving training speed and stability.\n",
    "\n",
    "Equal Weighting of Features: By scaling features to the same range, you ensure that each feature contributes proportionally to the model’s predictions, preventing the model from being biased toward a particular feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db67eedf-1321-4319-9b58-459c4daa3e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c00726e-2b3b-4581-82d7-2ead186a8ac1",
   "metadata": {},
   "source": [
    "22. How do we perform scaling in Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5acc00-0fce-4dfd-bedc-4f06e0024421",
   "metadata": {},
   "source": [
    "In Python, feature scaling can be performed easily using the sklearn.preprocessing module, which provides several tools for scaling data. The most commonly used scalers are:\n",
    "\n",
    "MinMaxScaler (Normalization)\n",
    "\n",
    "StandardScaler (Standardization)\n",
    "\n",
    "RobustScaler (For handling outliers)\n",
    "\n",
    "Normalizer (Scaling each sample individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a887b-b89d-4c3b-8434-12e564b99a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cff87ad0-a974-4e3a-b027-95bdfc190937",
   "metadata": {},
   "source": [
    "25. Explain data encoding?\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97853e7-bcdd-4085-b51b-b8aa23750789",
   "metadata": {},
   "source": [
    "Data encoding is the process of converting categorical data into a numerical format so that it can be fed into machine learning algorithms. Since most machine learning algorithms require numerical input, encoding is necessary to transform non-numeric categories (such as \"Male\", \"Female\", or \"Red\", \"Blue\", \"Green\") into a numerical representation.\n",
    "\n",
    "There are different types of encoding techniques used for categorical variables, depending on the nature of the data and the algorithm being used. The main encoding techniques are:\n",
    "\n",
    "1. Label Encoding\n",
    "2. One-Hot Encoding\n",
    "3. Binary Encoding\n",
    "4. Frequency / Count Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505861b-4e77-46e3-8a48-7ab1ee7476ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
